{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "PC_Building_Assistant.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "UyGvp7EIviGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460e08e2-8a8c-4c5f-926d-dd34552d7339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'watchful-net-416319'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import logging\n",
        "import threading\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "from sklearn.cluster import AgglomerativeClustering\n"
      ],
      "metadata": {
        "id": "8E_0Mg2ovkdJ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib seaborn\n",
        "!pip install google-cloud-storage\n",
        "!pip install google-cloud-pubsub\n"
      ],
      "metadata": {
        "id": "Uirp1fzniFZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd25f45-a3ec-4100-d20c-cba85442c3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global df_cpu, df_gpu, df_ram, df_ssd, df_hdd"
      ],
      "metadata": {
        "id": "YLjq3wXO7IJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# Initialize the GCS client\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "# Define your GCS bucket name\n",
        "bucket_name = 'pc-component-files'\n",
        "\n",
        "# Get the bucket\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "# List files in your bucket if needed\n",
        "blobs = bucket.list_blobs()\n",
        "print(\"Files in bucket:\")\n",
        "for blob in blobs:\n",
        "    print(blob.name)\n",
        "\n",
        "# Function to download a file from GCS to Colab environment\n",
        "def download_from_gcs(source_blob_name, destination_file_name):\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "    print(f\"Downloaded storage object {source_blob_name} from bucket {bucket_name} to local file {destination_file_name}.\")\n",
        "\n",
        "# Example: Download a specific file\n",
        "download_from_gcs('edit google doc csv files/cleaned_CPU_UB.csv', '/content/cleaned_CPU_UB.csv')\n",
        "download_from_gcs('edit google doc csv files/cleaned_GPU_UB.csv', '/content/cleaned_GPU_UB.csv')\n",
        "download_from_gcs('edit google doc csv files/cleaned_SSD_UB.csv', '/content/cleaned_SSD_UB.csv')\n",
        "download_from_gcs('edit google doc csv files/cleaned_HDD_UB.csv', '/content/cleaned_HDD_UB.csv')\n",
        "download_from_gcs('edit google doc csv files/cleaned_RAM_UB.csv', '/content/cleaned_RAM_UB.csv')\n",
        "\n",
        "download_from_gcs('clustered dataset/cleaned_CPU_UB_clustered.csv', '/content/cleaned_CPU_UB_clustered.csv')\n",
        "download_from_gcs('clustered dataset/cleaned_GPU_UB_clustered.csv', '/content/cleaned_GPU_UB_clustered.csv')\n",
        "download_from_gcs('clustered dataset/cleaned_HDD_UB_clustered.csv', '/content/cleaned_HDD_UB_clustered.csv')\n",
        "download_from_gcs('clustered dataset/cleaned_RAM_UB_clustered.csv', '/content/cleaned_RAM_UB_clustered.csv')\n",
        "download_from_gcs('clustered dataset/cleaned_SSD_UB_clustered.csv', '/content/cleaned_SSD_UB_clustered.csv')"
      ],
      "metadata": {
        "id": "4L_OfZ9BJ93g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read each CSV file into a DataFrame\n",
        "df_cpu = pd.read_csv('/content/cleaned_CPU_UB.csv')\n",
        "df_gpu = pd.read_csv('/content/cleaned_GPU_UB.csv')\n",
        "df_ssd = pd.read_csv('/content/cleaned_SSD_UB.csv')\n",
        "df_hdd = pd.read_csv('/content/cleaned_HDD_UB.csv')\n",
        "df_ram = pd.read_csv('/content/cleaned_RAM_UB.csv')\n",
        "\n",
        "print(\"CPU Data:\")\n",
        "print(df_cpu.head())\n",
        "print(\"\\nGPU Data:\")\n",
        "print(df_gpu.head())\n",
        "print(\"\\nSSD Data:\")\n",
        "print(df_ssd.head())\n",
        "print(\"\\nHDD Data:\")\n",
        "print(df_hdd.head())\n",
        "print(\"\\nRAM Data:\")\n",
        "print(df_ram.head())\n"
      ],
      "metadata": {
        "id": "uVKPZRlhKvqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_CPU_UB = pd.read_csv('/content/cleaned_CPU_UB_clustered.csv')\n",
        "cleaned_GPU_UB = pd.read_csv('/content/cleaned_GPU_UB_clustered.csv')\n",
        "cleaned_SSD_UB = pd.read_csv('/content/cleaned_HDD_UB_clustered.csv')\n",
        "cleaned_HDD_UB = pd.read_csv('/content/cleaned_RAM_UB_clustered.csv')\n",
        "cleaned_RAM_UB = pd.read_csv('/content/cleaned_SSD_UB_clustered.csv')\n",
        "\n",
        "print(\"CPU Data:\")\n",
        "print(cleaned_CPU_UB.head())\n",
        "print(\"\\nGPU Data:\")\n",
        "print(cleaned_GPU_UB.head())\n",
        "print(\"\\nSSD Data:\")\n",
        "print(cleaned_SSD_UB.head())\n",
        "print(\"\\nHDD Data:\")\n",
        "print(cleaned_HDD_UB.head())\n",
        "print(\"\\nRAM Data:\")\n",
        "print(cleaned_RAM_UB.head())"
      ],
      "metadata": {
        "id": "Xu_IDdXIFAiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation(file_path):\n",
        "    # Read the dataset from the given file path\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Select only numeric columns for correlation calculation\n",
        "    numeric_data = data.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    correlation_matrix = numeric_data.corr()\n",
        "\n",
        "    return correlation_matrix\n",
        "\n",
        "# List of downloaded file paths\n",
        "file_paths = [\n",
        "    '/content/cleaned_CPU_UB_clustered.csv',\n",
        "    '/content/cleaned_GPU_UB_clustered.csv',\n",
        "    '/content/cleaned_HDD_UB_clustered.csv',\n",
        "    '/content/cleaned_RAM_UB_clustered.csv',\n",
        "    '/content/cleaned_SSD_UB_clustered.csv'\n",
        "]\n",
        "\n",
        "# Iterate through each file path\n",
        "for file_path in file_paths:\n",
        "    print(f\"Correlation matrix for {file_path}:\")\n",
        "    correlation_matrix = calculate_correlation(file_path)\n",
        "    print(correlation_matrix)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "QOm9Cpkyv5g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CPU_benchmark_data = cleaned_CPU_UB[['Benchmark']]\n",
        "GPU_benchmark_data = cleaned_GPU_UB[['Benchmark']]\n",
        "HDD_benchmark_data = cleaned_HDD_UB[['Benchmark']]\n",
        "RAM_benchmark_data = cleaned_RAM_UB[['Benchmark']]\n",
        "SSD_benchmark_data = cleaned_SSD_UB[['Benchmark']]"
      ],
      "metadata": {
        "id": "p5ef8BiWqqCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "1CY-sQDdzKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "CPU_benchmark_data_scaled = scaler.fit_transform(CPU_benchmark_data)\n",
        "GPU_benchmark_data_scaled = scaler.fit_transform(GPU_benchmark_data)\n",
        "SSD_benchmark_data_scaled = scaler.fit_transform(SSD_benchmark_data)\n",
        "HDD_benchmark_data_scaled = scaler.fit_transform(HDD_benchmark_data)\n",
        "RAM_benchmark_data_scaled = scaler.fit_transform(RAM_benchmark_data)"
      ],
      "metadata": {
        "id": "9bME_t6ykded"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow plot, choose an appropriate value for K and perform K-means clustering\n",
        "k = 4  # Adjust this based on your Elbow plot observation\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "cleaned_CPU_UB['Cluster'] = kmeans.fit_predict(CPU_benchmark_data_scaled)"
      ],
      "metadata": {
        "id": "aEaPDzRIkdld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow plot, choose an appropriate value for K and perform K-means clustering\n",
        "k = 4  # Adjust this based on your Elbow plot observation\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "cleaned_GPU_UB['Cluster'] = kmeans.fit_predict(GPU_benchmark_data_scaled)"
      ],
      "metadata": {
        "id": "Yv-C_-a9lOCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow plot, choose an appropriate value for K and perform K-means clustering\n",
        "k = 4  # Adjust this based on your Elbow plot observation\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "cleaned_HDD_UB['Cluster'] = kmeans.fit_predict(HDD_benchmark_data_scaled)"
      ],
      "metadata": {
        "id": "KRzO6l82lOgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow plot, choose an appropriate value for K and perform K-means clustering\n",
        "k = 4  # Adjust this based on your Elbow plot observation\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "cleaned_SSD_UB['Cluster'] = kmeans.fit_predict(SSD_benchmark_data_scaled)"
      ],
      "metadata": {
        "id": "7rRxb7aOlTNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the Elbow plot, choose an appropriate value for K and perform K-means clustering\n",
        "k = 4  # Adjust this based on your Elbow plot observation\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "cleaned_RAM_UB['Cluster'] = kmeans.fit_predict(RAM_benchmark_data_scaled)"
      ],
      "metadata": {
        "id": "Mcf1cNDElW-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKI3GedmZxo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting CPU benchmark data\n",
        "CPU_train, CPU_test = train_test_split(CPU_benchmark_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting GPU benchmark data\n",
        "GPU_train, GPU_test = train_test_split(GPU_benchmark_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting SSD benchmark data\n",
        "SSD_train, SSD_test = train_test_split(SSD_benchmark_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting HDD benchmark data\n",
        "HDD_train, HDD_test = train_test_split(HDD_benchmark_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Splitting RAM benchmark data\n",
        "RAM_train, RAM_test = train_test_split(RAM_benchmark_data_scaled, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8G9n5ehumR3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply clustering and integrate labels\n",
        "def apply_clustering_and_integrate(X_train):\n",
        "    # Apply DBSCAN clustering\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    clusters_dbscan = dbscan.fit_predict(X_train)\n",
        "\n",
        "    # Apply Hierarchical clustering\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=4)  # Adjust based on analysis\n",
        "    clusters_hierarchical = hierarchical.fit_predict(X_train)\n",
        "\n",
        "    # Integrate DBSCAN cluster labels into the dataset\n",
        "    X_train_with_dbscan = np.column_stack((X_train, clusters_dbscan))\n",
        "\n",
        "    # Integrate Hierarchical cluster labels into the dataset\n",
        "    X_train_with_hierarchical = np.column_stack((X_train, clusters_hierarchical))\n",
        "\n",
        "    return X_train_with_dbscan, X_train_with_hierarchical\n"
      ],
      "metadata": {
        "id": "nwU9XZ2WnLV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying clustering to each training dataset and integrating labels\n",
        "CPU_train_with_dbscan, CPU_train_with_hierarchical = apply_clustering_and_integrate(CPU_train)\n",
        "GPU_train_with_dbscan, GPU_train_with_hierarchical = apply_clustering_and_integrate(GPU_train)\n",
        "SSD_train_with_dbscan, SSD_train_with_hierarchical = apply_clustering_and_integrate(SSD_train)\n",
        "HDD_train_with_dbscan, HDD_train_with_hierarchical = apply_clustering_and_integrate(HDD_train)\n",
        "RAM_train_with_dbscan, RAM_train_with_hierarchical = apply_clustering_and_integrate(RAM_train)"
      ],
      "metadata": {
        "id": "NLSpmisBnkaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrames for each dataset with hierarchical cluster labels\n",
        "CPU_train_df_hier = pd.DataFrame(CPU_train_with_hierarchical, columns=['Scaled_Benchmark', 'Hierarchical_Cluster'])\n",
        "GPU_train_df_hier = pd.DataFrame(GPU_train_with_hierarchical, columns=['Scaled_Benchmark', 'Hierarchical_Cluster'])\n",
        "SSD_train_df_hier = pd.DataFrame(SSD_train_with_hierarchical, columns=['Scaled_Benchmark', 'Hierarchical_Cluster'])\n",
        "HDD_train_df_hier = pd.DataFrame(HDD_train_with_hierarchical, columns=['Scaled_Benchmark', 'Hierarchical_Cluster'])\n",
        "RAM_train_df_hier = pd.DataFrame(RAM_train_with_hierarchical, columns=['Scaled_Benchmark', 'Hierarchical_Cluster'])\n",
        "\n",
        "# Display the first few rows for each DataFrame\n",
        "print(\"CPU Data (Hierarchical Clustering):\")\n",
        "print(CPU_train_df_hier.head())\n",
        "print(\"\\nGPU Data (Hierarchical Clustering):\")\n",
        "print(GPU_train_df_hier.head())\n",
        "print(\"\\nSSD Data (Hierarchical Clustering):\")\n",
        "print(SSD_train_df_hier.head())\n",
        "print(\"\\nHDD Data (Hierarchical Clustering):\")\n",
        "print(HDD_train_df_hier.head())\n",
        "print(\"\\nRAM Data (Hierarchical Clustering):\")\n",
        "print(RAM_train_df_hier.head())\n"
      ],
      "metadata": {
        "id": "LCdourI00wri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clusters(X, labels, title='Clustering Visualization'):\n",
        "    # Check if the data is 1D or 2D+\n",
        "    if X.ndim > 1 and X.shape[1] > 1:\n",
        "        # If data is multidimensional, apply PCA to reduce to two dimensions\n",
        "        pca = PCA(n_components=2)\n",
        "        X_transformed = pca.fit_transform(X)\n",
        "        x_axis = X_transformed[:, 0]\n",
        "        y_axis = X_transformed[:, 1]\n",
        "    else:\n",
        "        # If data is 1D, plot directly or duplicate the 1D data for x and use zeros for y\n",
        "        if X.ndim > 1:\n",
        "            x_axis = X[:, 0]\n",
        "        else:\n",
        "            x_axis = X  # If X is a 1D array already\n",
        "        y_axis = np.zeros(len(x_axis))  # Use zeros for y-axis\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(x=x_axis, y=y_axis, hue=labels, palette='viridis', legend='full', s=100)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Scaled Benchmark' if X.ndim == 1 or X.shape[1] == 1 else 'Dimension 1')\n",
        "    plt.ylabel('Dimension 2' if X.ndim > 1 and X.shape[1] > 1 else '')\n",
        "    plt.yticks([])  # Hide y-axis ticks for 1D data\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ywo6PS0gPRW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_print_silhouette_score(X, labels, clustering_type, data_label):\n",
        "    # Filter out noise for DBSCAN (-1 labels)\n",
        "    if clustering_type == 'DBSCAN':\n",
        "        filtered_labels = labels[labels != -1]\n",
        "        if len(np.unique(filtered_labels)) < 2:\n",
        "            print(f\"Silhouette Score for {data_label} with {clustering_type}: Cannot be computed (less than 2 clusters).\")\n",
        "            return\n",
        "        filtered_X = X[labels != -1, :]\n",
        "    else:\n",
        "        filtered_labels = labels\n",
        "        filtered_X = X\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    score = silhouette_score(filtered_X, filtered_labels)\n",
        "    print(f\"Silhouette Score for {data_label} with {clustering_type}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "oKsTFle0QWP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming *_train are your datasets and *_train_df['DBSCAN_Cluster'], *_train_df_hier['Hierarchical_Cluster']\n",
        "# contain the respective clustering labels\n",
        "# For Hierarchical clustering results\n",
        "calculate_and_print_silhouette_score(CPU_train, CPU_train_df_hier['Hierarchical_Cluster'], 'Hierarchical', 'CPU')\n",
        "calculate_and_print_silhouette_score(GPU_train, GPU_train_df_hier['Hierarchical_Cluster'], 'Hierarchical', 'GPU')\n",
        "calculate_and_print_silhouette_score(HDD_train, HDD_train_df_hier['Hierarchical_Cluster'], 'Hierarchical', 'HDD')\n",
        "calculate_and_print_silhouette_score(SSD_train, SSD_train_df_hier['Hierarchical_Cluster'], 'Hierarchical', 'SSD')\n",
        "calculate_and_print_silhouette_score(RAM_train, RAM_train_df_hier['Hierarchical_Cluster'], 'Hierarchical', 'RAM')\n",
        "\n",
        "# Continue for SSD, HDD, RAM\n"
      ],
      "metadata": {
        "id": "SSxe-xs9QgXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all column names in the DataFrame to verify the correct column names\n",
        "print(df_cpu.columns)\n"
      ],
      "metadata": {
        "id": "PCx4zoewiKhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU RECOMMENDATION SYSTEM\n"
      ],
      "metadata": {
        "id": "nu6azEDcG0XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = ['Benchmark']  # Assuming these are correct\n",
        "categorical_features = ['Model_Type','Tier', 'Processor_Number']  # Correct based on actual column names in DataFrame\n",
        "\n",
        "for feature in numeric_features:\n",
        "    if df_cpu[feature].dtype == 'object':\n",
        "        df_cpu[feature] = pd.to_numeric(df_cpu[feature], errors='coerce')\n",
        "        # The print statements should be indented to be part of the if block\n",
        "        print(f\"Non-numeric values found in {feature}:\")\n",
        "        print(df_cpu[feature].unique())  # This will show all unique values in the column\n",
        "\n",
        "# Define your transformations\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer\n",
        "preprocessor_cpu = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Now you can proceed to transform your data\n",
        "X_processed_cpu = preprocessor_cpu.fit_transform(df_cpu)\n",
        "\n",
        "# If you need the transformed DataFrame (for example, to check or use the transformed features)\n",
        "columns_transformed = (numeric_features +\n",
        "                       list(preprocessor_cpu.named_transformers_['cat'].get_feature_names_out(categorical_features)))\n",
        "X_processed_df_cpu = pd.DataFrame(X_processed_cpu.toarray(), columns=columns_transformed)\n",
        "print(X_processed_df_cpu)\n",
        "\n",
        "# Assuming X_processed_df is your processed DataFrame\n",
        "output_file_path = 'processed_cpu_data.csv'  # Specify your file path here\n",
        "X_processed_df_cpu.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "6WeFSg3BVQOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_dense to a numpy array if it isn't already one\n",
        "X_dense_cpu = X_processed_cpu.toarray() if hasattr(X_processed_cpu, \"toarray\") else np.array(X_processed_cpu)\n",
        "\n",
        "\n",
        "# Replace NaN values with the mean of each column, handling columns with all NaNs separately\n",
        "for i in range(X_dense_cpu.shape[1]):\n",
        "    column = X_dense_cpu[:, i]\n",
        "    nan_mask = np.isnan(column)\n",
        "    if nan_mask.all():\n",
        "        X_dense_cpu[:, i] = 0  # Replace all NaN columns with 0 or another appropriate value\n",
        "    else:\n",
        "        column_mean = np.nanmean(column)\n",
        "        column[nan_mask] = column_mean  # Replace NaNs in the column with the mean of the non-NaN values\n",
        "\n",
        "\n",
        "# Ensure no infinite values exist, replacing them if they do\n",
        "inf_mask = np.isinf(X_dense_cpu)\n",
        "X_dense_cpu[inf_mask] = np.nan  # Convert infinities to NaN\n",
        "np.nan_to_num(X_dense_cpu, copy=False, nan=0.0)  # Then replace NaNs with 0 (or another value)\n",
        "\n",
        "\n",
        "# Now proceed with hierarchical clustering\n",
        "linked = linkage(X_dense_cpu, 'ward')\n",
        "labelList = range(1, 11)\n",
        "\n",
        "# Generate labels dynamically based on the number of rows in X_dense\n",
        "labelList = [f\"Point {i+1}\" for i in range(X_dense_cpu.shape[0])]\n",
        "\n"
      ],
      "metadata": {
        "id": "nPfvNpvFhdYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_cpu is your DataFrame containing the data\n",
        "# Define which are your numeric and categorical features correctly\n",
        "numeric_features = ['Benchmark']  # This should be your actual numeric feature(s)\n",
        "categorical_features = ['Model_Type', 'Tier', 'Processor_Number']  # Including 'Tier' and 'Processor_Number' as categorical\n",
        "\n",
        "# Create pipelines for both numeric and categorical preprocessing\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer with appropriate transformations\n",
        "preprocessor_cpu = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed_cpu = preprocessor_cpu.fit_transform(df_cpu)  # No need to call .toarray() here yet\n",
        "\n",
        "# If you need a dense format, especially for hierarchical clustering, convert to array\n",
        "X_processed_dense_cpu = X_processed_cpu.toarray()\n",
        "\n",
        "# Hierarchical Clustering to create a linkage matrix 'Z'\n",
        "Z = linkage(X_processed_dense_cpu, method='ward')\n",
        "\n",
        "\n",
        "# Find the largest distance gap in the dendrogram to suggest an optimal distance\n",
        "distances = Z[:, 2]\n",
        "distance_diffs = np.diff(-np.sort(distances))\n",
        "idx_of_largest_gap = np.argmax(distance_diffs)\n",
        "optimal_distance = distances[idx_of_largest_gap]\n",
        "clusters = fcluster(Z, optimal_distance, criterion='distance')\n",
        "\n",
        "# Find the optimal number of clusters using silhouette scores\n",
        "max_silhouette_score = -1\n",
        "optimal_num_clusters = 81\n",
        "\n",
        "##for num_clusters in range(2, len(np.unique(clusters)) + 1):\n",
        "  ##  cluster_labels = fcluster(Z, num_clusters, criterion='maxclust')\n",
        "    ##silhouette_avg = silhouette_score(X_processed, cluster_labels)\n",
        "    ##print(f'For n_clusters = {num_clusters}, the average silhouette_score is : {silhouette_avg}')\n",
        "    ##if silhouette_avg > max_silhouette_score:\n",
        "      ##  max_silhouette_score = silhouette_avg\n",
        "        #optimal_num_clusters = 81\n",
        "\n",
        "print(f\"The optimal number of clusters based on silhouette score is: {optimal_num_clusters}\") #81 clusters"
      ],
      "metadata": {
        "id": "YLPQWYExuWNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Agglomerative Clustering\n",
        "cluster_cpu= AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster_labels_cpu = cluster_cpu.fit_predict(X_processed_dense)  # Use X_processed_dense here\n",
        "\n",
        "print(cluster_labels_cpu)"
      ],
      "metadata": {
        "id": "DPx_ahluhZNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows in df_cpu: {len(df_cpu)}\")"
      ],
      "metadata": {
        "id": "XE_TTmlFk5r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: If you dropped NA values before clustering\n",
        "df_cpu_cleaned = df_cpu.dropna(subset=['Benchmark', 'Model_Type', 'Tier', 'Processor_Number'])\n",
        "\n",
        "# Assuming the generation of cluster_labels here aligns with your data\n",
        "if len(cluster_labels_cpu) == len(df_cpu):\n",
        "    df_cpu['cluster'] = cluster_labels_cpu\n",
        "    print(\"Cluster labels successfully assigned.\")\n",
        "else:\n",
        "    print(f\"Mismatch detected: {len(cluster_labels_cpu)} cluster labels vs {len(df_cpu)} rows in DataFrame.\")\n",
        "    # You might need to revisit the clustering step or ensure preprocessing steps are aligned\n"
      ],
      "metadata": {
        "id": "shgqg5UFO3fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_cpu is your DataFrame\n",
        "# First, convert columns that should be numeric but are 'object' type\n",
        "#for col in df_cpu.columns:\n",
        " #   if df_cpu[col].dtype == 'object':\n",
        "        # Attempt to convert to numeric, coerce errors will turn non-convertible values into NaN\n",
        "  #      df_cpu[col] = pd.to_numeric(df_cpu[col], errors='coerce')\n",
        "\n",
        "# Now exclude any remaining non-numeric columns before grouping\n",
        "#numeric_cols = df_cpu.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculating centroids of numeric columns only\n",
        "#centroids = numeric_cols.groupby('cluster').mean()\n",
        "\n",
        "#print(centroids)"
      ],
      "metadata": {
        "id": "kNanc_NYkmDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpu['cluster_cpu'] = cluster_labels_cpu\n"
      ],
      "metadata": {
        "id": "NDW8r6qPvx-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU RECOMMENDATION SYSTEM"
      ],
      "metadata": {
        "id": "K23pYoeVHBy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_gpu.columns)"
      ],
      "metadata": {
        "id": "nSEn3JczRe3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the column names as per your GPU dataset\n",
        "numeric_features_gpu = ['Benchmark']  # Assuming Benchmark is a numeric feature for GPUs as well\n",
        "categorical_features_gpu = ['GPU_Model_Type', 'VRAM_Size', 'GPU_Model_Tier']  # Example categorical features for GPUs, adjust as needed\n",
        "\n",
        "for feature in numeric_features_gpu:\n",
        "    if df_gpu[feature].dtype == 'object':\n",
        "        df_gpu[feature] = pd.to_numeric(df_gpu[feature], errors='coerce')\n",
        "        print(f\"Non-numeric values found in {feature}:\")\n",
        "        print(df_gpu[feature].unique())\n",
        "\n",
        "# Define your transformations for GPU data\n",
        "numeric_transformer_gpu = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer_gpu = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer for GPU data\n",
        "preprocessor_gpu = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_gpu, numeric_features_gpu),\n",
        "    ('cat', categorical_transformer_gpu, categorical_features_gpu)\n",
        "])\n",
        "\n",
        "# Now you can proceed to transform your GPU data\n",
        "X_processed_gpu = preprocessor_gpu.fit_transform(df_gpu)\n",
        "\n",
        "# If you need the transformed DataFrame (for example, to check or use the transformed features for GPUs)\n",
        "columns_transformed_gpu = (numeric_features_gpu +\n",
        "                           list(preprocessor_gpu.named_transformers_['cat'].get_feature_names_out(categorical_features_gpu)))\n",
        "X_processed_gpu_df = pd.DataFrame(X_processed_gpu.toarray(), columns=columns_transformed_gpu)\n",
        "print(X_processed_gpu_df)\n",
        "\n",
        "# Assuming X_processed_gpu_df is your processed DataFrame for GPU data\n",
        "output_file_path_gpu = 'processed_gpu_data.csv'  # Specify your file path here\n",
        "X_processed_gpu_df.to_csv(output_file_path_gpu, index=False)"
      ],
      "metadata": {
        "id": "b4587hIZUgQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_processed_gpu to a numpy array if it isn't already one\n",
        "X_dense_gpu = X_processed_gpu.toarray() if hasattr(X_processed_gpu, \"toarray\") else np.array(X_processed_gpu)\n",
        "\n",
        "# Replace NaN values with the mean of each column, handling columns with all NaNs separately for GPU data\n",
        "for i in range(X_dense_gpu.shape[1]):\n",
        "    column = X_dense_gpu[:, i]\n",
        "    nan_mask = np.isnan(column)\n",
        "    if nan_mask.all():\n",
        "        X_dense_gpu[:, i] = 0  # Replace all NaN columns with 0 or another appropriate value\n",
        "    else:\n",
        "        column_mean = np.nanmean(column)\n",
        "        column[nan_mask] = column_mean  # Replace NaNs in the column with the mean of the non-NaN values\n",
        "\n",
        "# Ensure no infinite values exist, replacing them if they do for GPU data\n",
        "inf_mask = np.isinf(X_dense_gpu)\n",
        "X_dense_gpu[inf_mask] = np.nan  # Convert infinities to NaN\n",
        "np.nan_to_num(X_dense_gpu, copy=False, nan=0.0)  # Then replace NaNs with 0 (or another value)\n",
        "\n",
        "# Now proceed with hierarchical clustering for GPU data\n",
        "linked_gpu = linkage(X_dense_gpu, 'ward')\n",
        "labelList_gpu = range(1, 11)\n",
        "\n",
        "# Generate labels dynamically based on the number of rows in X_dense_gpu\n",
        "labelList_gpu = [f\"Point {i+1}\" for i in range(X_dense_gpu.shape[0])]"
      ],
      "metadata": {
        "id": "xzGT_R-iHoxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_gpu is your DataFrame containing the GPU data\n",
        "# Adjust your numeric and categorical features accordingly\n",
        "numeric_features_gpu = ['Benchmark']  # Benchmark remains a numeric feature\n",
        "categorical_features_gpu = ['GPU_Model_Type', 'VRAM_Size', 'GPU_Model_Tier']  # Adjusted for GPU-specific features\n",
        "\n",
        "# Create pipelines for both numeric and categorical preprocessing for GPU\n",
        "numeric_transformer_gpu = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer_gpu = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer with appropriate transformations for GPU\n",
        "preprocessor_gpu = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_gpu, numeric_features_gpu),\n",
        "    ('cat', categorical_transformer_gpu, categorical_features_gpu)\n",
        "])\n",
        "\n",
        "# Apply preprocessing to the GPU data\n",
        "X_processed_gpu = preprocessor_gpu.fit_transform(df_gpu)  # This gets you the transformed data\n",
        "\n",
        "# Convert to dense format if necessary, especially for hierarchical clustering\n",
        "X_processed_dense_gpu = X_processed_gpu.toarray()\n",
        "\n",
        "# Hierarchical Clustering to create a linkage matrix 'Z' for GPU data\n",
        "Z_gpu = linkage(X_processed_dense_gpu, method='ward')\n",
        "\n",
        "# Plot the dendrogram for visual inspection for GPU data\n",
        "#plt.figure(figsize=(10, 7))\n",
        "#dendrogram(Z_gpu)\n",
        "#plt.title('Hierarchical Clustering Dendrogram for GPU Data')\n",
        "#plt.xlabel('Sample index')\n",
        "#plt.ylabel('Distance')\n",
        "#plt.show()\n",
        "\n",
        "# Suggesting an optimal distance and finding the number of clusters for GPU data\n",
        "distances_gpu = Z_gpu[:, 2]\n",
        "distance_diffs_gpu = np.diff(-np.sort(distances_gpu))\n",
        "idx_of_largest_gap_gpu = np.argmax(distance_diffs_gpu)\n",
        "optimal_distance_gpu = distances_gpu[idx_of_largest_gap_gpu]\n",
        "clusters_gpu = fcluster(Z_gpu, optimal_distance_gpu, criterion='distance')\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette scores for GPU data\n",
        "max_silhouette_score_gpu = -1\n",
        "optimal_num_clusters_gpu = 263\n",
        "\n",
        "#for num_clusters in range(2, len(np.unique(clusters_gpu)) + 1):\n",
        "#    cluster_labels_gpu = fcluster(Z_gpu, num_clusters, criterion='maxclust')\n",
        "#    silhouette_avg_gpu = silhouette_score(X_processed_gpu, cluster_labels_gpu)\n",
        " #   print(f'For n_clusters = {num_clusters}, the average silhouette_score is : {silhouette_avg_gpu}')\n",
        "  #  if silhouette_avg_gpu > max_silhouette_score_gpu:\n",
        "   #     max_silhouette_score_gpu = silhouette_avg_gpu\n",
        "    #    optimal_num_clusters_gpu = num_clusters\n",
        "\n",
        "print(f\"The optimal number of clusters based on silhouette score for GPU data is: {optimal_num_clusters_gpu}\")#\n"
      ],
      "metadata": {
        "id": "RRztCNFyHsgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Agglomerative Clustering to GPU data\n",
        "cluster_gpu = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster_labels_gpu = cluster_gpu.fit_predict(X_processed_dense_gpu)  # Use the processed dense GPU data here\n",
        "\n",
        "print(cluster_labels_gpu)\n"
      ],
      "metadata": {
        "id": "kFWr321RHsrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_cpu is the original DataFrame and cluster_labels contains the cluster assignments\n",
        "df_gpu['cluster_gpu'] = cluster_labels_gpu\n",
        "\n",
        "# Calculating centroids\n",
        "#GPU_centroids = df_gpu.groupby('cluster_gpu').mean()\n",
        "\n",
        "#print(GPU_centroids)"
      ],
      "metadata": {
        "id": "enWG9RZxHtbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM RECOMMENDATION SYSTEM"
      ],
      "metadata": {
        "id": "rF9mZcU3HFhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ram.columns)"
      ],
      "metadata": {
        "id": "SoLqal8JHQ-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ram is your RAM dataset DataFrame\n",
        "# Load your RAM dataset here, for example:\n",
        "# df_ram = pd.read_csv('path_to_your_ram_dataset.csv')\n",
        "\n",
        "# Define numeric and categorical features for RAM\n",
        "numeric_features_ram = ['Benchmark']\n",
        "categorical_features_ram = ['Type_Speed', 'CAS_Latency', 'RAM_Sticks', 'GB_Amount']\n",
        "\n",
        "# Preprocess numeric features to ensure they are numeric, especially if they are imported as strings\n",
        "for feature in numeric_features_ram:\n",
        "    if df_ram[feature].dtype == 'object':\n",
        "        df_ram[feature] = pd.to_numeric(df_ram[feature], errors='coerce')\n",
        "        print(f\"Non-numeric values found in {feature}:\")\n",
        "        print(df_ram[feature].unique())\n",
        "\n",
        "# Create preprocessing pipelines for RAM\n",
        "numeric_transformer_ram = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_ram = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer for RAM data\n",
        "preprocessor_ram = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer_ram, numeric_features_ram),\n",
        "        ('cat', categorical_transformer_ram, categorical_features_ram)])\n",
        "\n",
        "# Preprocess the RAM dataset\n",
        "X_processed_ram = preprocessor_ram.fit_transform(df_ram)\n",
        "\n",
        "# Convert the processed data into a dense DataFrame for easier handling and visualization\n",
        "columns_transformed_ram = numeric_features_ram + \\\n",
        "    list(preprocessor_ram.named_transformers_['cat'].get_feature_names_out(categorical_features_ram))\n",
        "X_processed_ram_df = pd.DataFrame(X_processed_ram.toarray(), columns=columns_transformed_ram)\n",
        "print(X_processed_ram_df)\n",
        "\n",
        "# Save the processed RAM DataFrame to a CSV file\n",
        "output_file_path_ram = 'processed_ram_data.csv'  # Specify your file path here\n",
        "X_processed_ram_df.to_csv(output_file_path_ram, index=False)\n"
      ],
      "metadata": {
        "id": "fxMdqLu4HuKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_processed_ram to a numpy array if it isn't already one\n",
        "X_dense_ram = X_processed_ram.toarray() if hasattr(X_processed_ram, \"toarray\") else np.array(X_processed_ram)\n",
        "\n",
        "# Replace NaN values with the mean of each column, handling columns with all NaNs separately for RAM data\n",
        "for i in range(X_dense_ram.shape[1]):\n",
        "    column = X_dense_ram[:, i]\n",
        "    nan_mask = np.isnan(column)\n",
        "    if nan_mask.all():\n",
        "        X_dense_ram[:, i] = 0  # Replace all NaN columns with 0 or another appropriate value\n",
        "    else:\n",
        "        column_mean = np.nanmean(column)\n",
        "        column[nan_mask] = column_mean  # Replace NaNs in the column with the mean of the non-NaN values\n",
        "\n",
        "# Ensure no infinite values exist, replacing them if they do for RAM data\n",
        "inf_mask = np.isinf(X_dense_ram)\n",
        "X_dense_ram[inf_mask] = np.nan  # Convert infinities to NaN\n",
        "np.nan_to_num(X_dense_ram, copy=False, nan=0.0)  # Then replace NaNs with 0 (or another value)\n",
        "\n",
        "# Now proceed with hierarchical clustering for RAM data\n",
        "linked_ram = linkage(X_dense_ram, 'ward')\n",
        "\n",
        "# Assuming you want to dynamically generate labels for a plot or analysis\n",
        "# Here we prepare labelList_ram for potentially labeling points in a plot\n",
        "labelList_ram = [f\"Point {i+1}\" for i in range(X_dense_ram.shape[0])]"
      ],
      "metadata": {
        "id": "r73qkfEeHuN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ram is your DataFrame containing the RAM data\n",
        "# Define numeric and categorical features for RAM\n",
        "numeric_features_ram = ['Benchmark']  # Benchmark is a numeric feature for RAM as well\n",
        "categorical_features_ram = ['Type_Speed', 'CAS_Latency', 'RAM_Sticks', 'GB_Amount']  # Specific features for RAM\n",
        "\n",
        "# Create pipelines for both numeric and categorical preprocessing for RAM\n",
        "numeric_transformer_ram = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_ram = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer with appropriate transformations for RAM\n",
        "preprocessor_ram = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_ram, numeric_features_ram),\n",
        "    ('cat', categorical_transformer_ram, categorical_features_ram)])\n",
        "\n",
        "# Apply preprocessing to the RAM data\n",
        "X_processed_ram = preprocessor_ram.fit_transform(df_ram)  # This gets you the transformed RAM data\n",
        "\n",
        "# Convert to dense format if necessary, especially for hierarchical clustering\n",
        "X_processed_dense_ram = X_processed_ram.toarray()\n",
        "\n",
        "# Hierarchical Clustering to create a linkage matrix 'Z' for RAM data\n",
        "Z_ram = linkage(X_processed_dense_ram, method='ward')\n",
        "\n",
        "# Plot the dendrogram for visual inspection for RAM data\n",
        "#plt.figure(figsize=(10, 7))\n",
        "#dendrogram(Z_ram)\n",
        "#plt.title('Hierarchical Clustering Dendrogram for RAM Data')\n",
        "#plt.xlabel('Sample index')\n",
        "#plt.ylabel('Distance')\n",
        "#plt.show()\n",
        "\n",
        "# Suggesting an optimal distance and finding the number of clusters for RAM data\n",
        "distances_ram = Z_ram[:, 2]\n",
        "distance_diffs_ram = np.diff(-np.sort(distances_ram))\n",
        "idx_of_largest_gap_ram = np.argmax(distance_diffs_ram)\n",
        "optimal_distance_ram = distances_ram[idx_of_largest_gap_ram]\n",
        "clusters_ram = fcluster(Z_ram, optimal_distance_ram, criterion='distance')\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette scores for RAM data\n",
        "max_silhouette_score_ram = -1\n",
        "optimal_num_clusters_ram = 59\n",
        "#for num_clusters_ram in range(2, len(np.unique(clusters_ram)) + 1):\n",
        " #   cluster_labels_ram = fcluster(Z_ram, num_clusters_ram, criterion='maxclust')\n",
        "  #  silhouette_avg_ram = silhouette_score(X_processed_ram, cluster_labels_ram)\n",
        "   # if silhouette_avg_ram > max_silhouette_score_ram:\n",
        "    #    max_silhouette_score_ram = silhouette_avg_ram\n",
        "     #   optimal_num_clusters_ram = num_clusters_ram\n",
        "\n",
        "print(f\"The optimal number of clusters based on silhouette score for RAM data is: {optimal_num_clusters_ram}\")"
      ],
      "metadata": {
        "id": "9KV3WEk1HuWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Agglomerative Clustering to RAM data\n",
        "cluster_ram = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster_labels_ram = cluster_ram.fit_predict(X_processed_dense_ram)  # Use the processed dense RAM data here\n",
        "\n",
        "print(cluster_labels_ram)\n"
      ],
      "metadata": {
        "id": "9KdiaZuBHuYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ram is the original DataFrame and cluster_labels_ram contains the cluster assignments\n",
        "df_ram['cluster_ram'] = cluster_labels_ram\n",
        "\n",
        "# Calculating centroids for RAM data\n",
        "#RAM_centroids = df_ram.groupby('cluster_ram').mean()\n",
        "\n",
        "#print(RAM_centroids)\n"
      ],
      "metadata": {
        "id": "aMgyU4cztfUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SSD RECOMMENDATION SYSTEM\n"
      ],
      "metadata": {
        "id": "43YH_TzdHH5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ssd.columns)"
      ],
      "metadata": {
        "id": "lIrx2JZHzdsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ssd is your SSD dataset DataFrame\n",
        "# Load your SSD dataset here, for example:\n",
        "# df_ssd = pd.read_csv('path_to_your_ssd_dataset.csv')\n",
        "\n",
        "# Define numeric and categorical features for SSD\n",
        "numeric_features_ssd = ['Benchmark']\n",
        "categorical_features_ssd = ['Size', 'Model_Type']\n",
        "\n",
        "# Preprocess numeric features to ensure they are numeric, especially if they are imported as strings\n",
        "for feature in numeric_features_ssd:\n",
        "    if df_ssd[feature].dtype == 'object':\n",
        "        df_ssd[feature] = pd.to_numeric(df_ssd[feature], errors='coerce')\n",
        "        print(f\"Non-numeric values found in {feature}:\")\n",
        "        print(df_ssd[feature].unique())\n",
        "\n",
        "# Create preprocessing pipelines for SSD\n",
        "numeric_transformer_ssd = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_ssd = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer for SSD data\n",
        "preprocessor_ssd = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer_ssd, numeric_features_ssd),\n",
        "        ('cat', categorical_transformer_ssd, categorical_features_ssd)])\n",
        "\n",
        "# Preprocess the SSD dataset\n",
        "X_processed_ssd = preprocessor_ssd.fit_transform(df_ssd)\n",
        "\n",
        "# Convert the processed data into a dense DataFrame for easier handling and visualization\n",
        "columns_transformed_ssd = numeric_features_ssd + \\\n",
        "    list(preprocessor_ssd.named_transformers_['cat'].get_feature_names_out(categorical_features_ssd))\n",
        "X_processed_ssd_df = pd.DataFrame(X_processed_ssd.toarray(), columns=columns_transformed_ssd)\n",
        "print(X_processed_ssd_df)\n",
        "\n",
        "# Save the processed SSD DataFrame to a CSV file\n",
        "output_file_path_ssd = 'processed_ssd_data.csv'  # Specify your file path here\n",
        "X_processed_ssd_df.to_csv(output_file_path_ssd, index=False)"
      ],
      "metadata": {
        "id": "ELaPf_9THRWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_processed_ssd to a numpy array if it isn't already one\n",
        "X_dense_ssd = X_processed_ssd.toarray() if hasattr(X_processed_ssd, \"toarray\") else np.array(X_processed_ssd)\n",
        "\n",
        "# Replace NaN values with the mean of each column, handling columns with all NaNs separately for SSD data\n",
        "for i in range(X_dense_ssd.shape[1]):\n",
        "    column = X_dense_ssd[:, i]\n",
        "    nan_mask = np.isnan(column)\n",
        "    if nan_mask.all():\n",
        "        X_dense_ssd[:, i] = 0  # Replace all NaN columns with 0 or another appropriate value\n",
        "    else:\n",
        "        column_mean = np.nanmean(column)\n",
        "        column[nan_mask] = column_mean  # Replace NaNs in the column with the mean of the non-NaN values\n",
        "\n",
        "# Ensure no infinite values exist, replacing them if they do for SSD data\n",
        "inf_mask = np.isinf(X_dense_ssd)\n",
        "X_dense_ssd[inf_mask] = np.nan  # Convert infinities to NaN\n",
        "np.nan_to_num(X_dense_ssd, copy=False, nan=0.0)  # Then replace NaNs with 0 (or another value)\n",
        "\n",
        "# Now proceed with hierarchical clustering for SSD data\n",
        "linked_ssd = linkage(X_dense_ssd, 'ward')\n",
        "\n",
        "# Assuming you want to dynamically generate labels for a plot or analysis\n",
        "# Here we prepare labelList_ssd for potentially labeling points in a plot\n",
        "labelList_ssd = [f\"Point {i+1}\" for i in range(X_dense_ssd.shape[0])]"
      ],
      "metadata": {
        "id": "Me-nD4OOHwNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ssd is your DataFrame containing the SSD data\n",
        "# Define numeric and categorical features for SSD\n",
        "numeric_features_ssd = ['Benchmark']  # Benchmark is a numeric feature for SSD as well\n",
        "categorical_features_ssd = ['Size', 'Model_Type']  # Specific features for SSD\n",
        "\n",
        "# Create pipelines for both numeric and categorical preprocessing for SSD\n",
        "numeric_transformer_ssd = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_ssd = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer with appropriate transformations for SSD\n",
        "preprocessor_ssd = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_ssd, numeric_features_ssd),\n",
        "    ('cat', categorical_transformer_ssd, categorical_features_ssd)])\n",
        "\n",
        "# Apply preprocessing to the SSD data\n",
        "X_processed_ssd = preprocessor_ssd.fit_transform(df_ssd)  # This gets you the transformed SSD data\n",
        "\n",
        "# Convert to dense format if necessary, especially for hierarchical clustering\n",
        "X_processed_dense_ssd = X_processed_ssd.toarray()\n",
        "\n",
        "# Hierarchical Clustering to create a linkage matrix 'Z' for SSD data\n",
        "Z_ssd = linkage(X_processed_dense_ssd, method='ward')\n",
        "\n",
        "# Plot the dendrogram for visual inspection for SSD data\n",
        "#plt.figure(figsize=(10, 7))\n",
        "#dendrogram(Z_ssd)\n",
        "#plt.title('Hierarchical Clustering Dendrogram for SSD Data')\n",
        "#plt.xlabel('Sample index')\n",
        "#plt.ylabel('Distance')\n",
        "#plt.show()\n",
        "\n",
        "# Suggesting an optimal distance and finding the number of clusters for SSD data\n",
        "distances_ssd = Z_ssd[:, 2]\n",
        "distance_diffs_ssd = np.diff(-np.sort(distances_ssd))\n",
        "idx_of_largest_gap_ssd = np.argmax(distance_diffs_ssd)\n",
        "optimal_distance_ssd = distances_ssd[idx_of_largest_gap_ssd]\n",
        "clusters_ssd = fcluster(Z_ssd, optimal_distance_ssd, criterion='distance')\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette scores for SSD data\n",
        "max_silhouette_score_ssd = -1\n",
        "optimal_num_clusters_ssd = 63\n",
        "#for num_clusters_ssd in range(2, len(np.unique(clusters_ssd)) + 1):\n",
        " #   cluster_labels_ssd = fcluster(Z_ssd, num_clusters_ssd, criterion='maxclust')\n",
        "  #  silhouette_avg_ssd = silhouette_score(X_processed_ssd, cluster_labels_ssd)\n",
        "   # if silhouette_avg_ssd > max_silhouette_score_ssd:\n",
        "    #    max_silhouette_score_ssd = silhouette_avg_ssd\n",
        "     #   optimal_num_clusters_ssd = num_clusters_ssd\n",
        "\n",
        "print(f\"The optimal number of clusters based on silhouette score for SSD data is: {optimal_num_clusters_ssd}\")\n"
      ],
      "metadata": {
        "id": "qQ2rytQzHwPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Agglomerative Clustering to SSD data\n",
        "cluster_ssd = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster_labels_ssd = cluster_ssd.fit_predict(X_processed_dense_ssd)  # Use the processed dense SSD data here\n",
        "\n",
        "print(cluster_labels_ssd)\n"
      ],
      "metadata": {
        "id": "-NZAZmrNHwSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_ssd is the original DataFrame and cluster_labels_ssd contains the cluster assignments\n",
        "df_ssd['cluster_ssd'] = cluster_labels_ssd\n",
        "\n",
        "# Calculating centroids for SSD data\n",
        "#SSD_centroids = df_ssd.groupby('cluster_ssd').mean()\n",
        "\n",
        "#print(SSD_centroids)\n",
        "\n"
      ],
      "metadata": {
        "id": "z2HpA--nHwbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HDD RECOMMENDATION SYSTEM"
      ],
      "metadata": {
        "id": "fa2rUWRkHNvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_hdd.columns)"
      ],
      "metadata": {
        "id": "akjhunlTzo_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_hdd is your HDD dataset DataFrame\n",
        "# Load your HDD dataset here, for example:\n",
        "# df_hdd = pd.read_csv('path_to_your_hdd_dataset.csv')\n",
        "\n",
        "# Define numeric and categorical features for HDD\n",
        "numeric_features_hdd = ['Benchmark']\n",
        "categorical_features_hdd = ['Storage_Size', 'Model_Name']\n",
        "\n",
        "# Preprocess numeric features to ensure they are numeric, especially if they are imported as strings\n",
        "for feature in numeric_features_hdd:\n",
        "    if df_hdd[feature].dtype == 'object':\n",
        "        df_hdd[feature] = pd.to_numeric(df_hdd[feature], errors='coerce')\n",
        "        print(f\"Non-numeric values found in {feature}:\")\n",
        "        print(df_hdd[feature].unique())\n",
        "\n",
        "# Create preprocessing pipelines for HDD\n",
        "numeric_transformer_hdd = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_hdd = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer for HDD data\n",
        "preprocessor_hdd = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer_hdd, numeric_features_hdd),\n",
        "        ('cat', categorical_transformer_hdd, categorical_features_hdd)])\n",
        "\n",
        "# Preprocess the HDD dataset\n",
        "X_processed_hdd = preprocessor_hdd.fit_transform(df_hdd)\n",
        "\n",
        "# Convert the processed data into a dense DataFrame for easier handling and visualization\n",
        "columns_transformed_hdd = numeric_features_hdd + \\\n",
        "    list(preprocessor_hdd.named_transformers_['cat'].get_feature_names_out(categorical_features_hdd))\n",
        "X_processed_hdd_df = pd.DataFrame(X_processed_hdd.toarray(), columns=columns_transformed_hdd)\n",
        "print(X_processed_hdd_df)\n",
        "\n",
        "# Save the processed HDD DataFrame to a CSV file\n",
        "output_file_path_hdd = 'processed_hdd_data.csv'  # Specify your file path here\n",
        "X_processed_hdd_df.to_csv(output_file_path_hdd, index=False)\n"
      ],
      "metadata": {
        "id": "1tuu8bdgzpEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_processed_hdd to a numpy array if it isn't already one\n",
        "X_dense_hdd = X_processed_hdd.toarray() if hasattr(X_processed_hdd, \"toarray\") else np.array(X_processed_hdd)\n",
        "\n",
        "# Replace NaN values with the mean of each column, handling columns with all NaNs separately for HDD data\n",
        "for i in range(X_dense_hdd.shape[1]):\n",
        "    column = X_dense_hdd[:, i]\n",
        "    nan_mask = np.isnan(column)\n",
        "    if nan_mask.all():\n",
        "        X_dense_hdd[:, i] = 0  # Replace all NaN columns with 0 or another appropriate value\n",
        "    else:\n",
        "        column_mean = np.nanmean(column)\n",
        "        column[nan_mask] = column_mean  # Replace NaNs in the column with the mean of the non-NaN values\n",
        "\n",
        "# Ensure no infinite values exist, replacing them if they do for HDD data\n",
        "inf_mask = np.isinf(X_dense_hdd)\n",
        "X_dense_hdd[inf_mask] = np.nan  # Convert infinities to NaN\n",
        "np.nan_to_num(X_dense_hdd, copy=False, nan=0.0)  # Then replace NaNs with 0 (or another value)\n",
        "\n",
        "# Now proceed with hierarchical clustering for HDD data\n",
        "linked_hdd = linkage(X_dense_hdd, 'ward')\n",
        "\n",
        "# Assuming you want to dynamically generate labels for a plot or analysis\n",
        "# Here we prepare labelList_hdd for potentially labeling points in a plot\n",
        "labelList_hdd = [f\"Point {i+1}\" for i in range(X_dense_hdd.shape[0])]\n"
      ],
      "metadata": {
        "id": "8Z0DKwbxHxx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_hdd is your DataFrame containing the HDD data\n",
        "# Define numeric and categorical features for HDD\n",
        "numeric_features_hdd = ['Benchmark']  # Benchmark is a numeric feature for HDD as well\n",
        "categorical_features_hdd = ['Storage_Size', 'Model_Name']  # Specific features for HDD\n",
        "\n",
        "# Create pipelines for both numeric and categorical preprocessing for HDD\n",
        "numeric_transformer_hdd = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer_hdd = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Create the ColumnTransformer with appropriate transformations for HDD\n",
        "preprocessor_hdd = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer_hdd, numeric_features_hdd),\n",
        "    ('cat', categorical_transformer_hdd, categorical_features_hdd)])\n",
        "\n",
        "# Apply preprocessing to the HDD data\n",
        "X_processed_hdd = preprocessor_hdd.fit_transform(df_hdd)  # This gets you the transformed HDD data\n",
        "\n",
        "# Convert to dense format if necessary, especially for hierarchical clustering\n",
        "X_processed_dense_hdd = X_processed_hdd.toarray()\n",
        "\n",
        "# Hierarchical Clustering to create a linkage matrix 'Z' for HDD data\n",
        "Z_hdd = linkage(X_processed_dense_hdd, method='ward')\n",
        "\n",
        "# Plot the dendrogram for visual inspection for HDD data\n",
        "#plt.figure(figsize=(10, 7))\n",
        "#dendrogram(Z_hdd)\n",
        "#plt.title('Hierarchical Clustering Dendrogram for HDD Data')\n",
        "#plt.xlabel('Sample index')\n",
        "#plt.ylabel('Distance')\n",
        "#plt.show()\n",
        "\n",
        "# Suggesting an optimal distance and finding the number of clusters for HDD data\n",
        "distances_hdd = Z_hdd[:, 2]\n",
        "distance_diffs_hdd = np.diff(-np.sort(distances_hdd))\n",
        "idx_of_largest_gap_hdd = np.argmax(distance_diffs_hdd)\n",
        "optimal_distance_hdd = distances_hdd[idx_of_largest_gap_hdd]\n",
        "clusters_hdd = fcluster(Z_hdd, optimal_distance_hdd, criterion='distance')\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette scores for HDD data\n",
        "max_silhouette_score_hdd = -1\n",
        "optimal_num_clusters_hdd = 2\n",
        "#for num_clusters_hdd in range(2, len(np.unique(clusters_hdd)) + 1):\n",
        " #   cluster_labels_hdd = fcluster(Z_hdd, num_clusters_hdd, criterion='maxclust')\n",
        "  #  silhouette_avg_hdd = silhouette_score(X_processed_dense_hdd, cluster_labels_hdd)\n",
        "   # if silhouette_avg_hdd > max_silhouette_score_hdd:\n",
        "    #    max_silhouette_score_hdd = silhouette_avg_hdd\n",
        "     #   optimal_num_clusters_hdd = num_clusters_hdd\n",
        "\n",
        "print(f\"The optimal number of clusters based on silhouette score for HDD data is: {optimal_num_clusters_hdd}\")\n"
      ],
      "metadata": {
        "id": "1baow4hDHx0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Agglomerative Clustering to HDD data\n",
        "cluster_hdd = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "cluster_labels_hdd = cluster_hdd.fit_predict(X_processed_dense_hdd)  # Use the processed dense HDD data here\n",
        "\n",
        "print(cluster_labels_hdd)"
      ],
      "metadata": {
        "id": "vAPJ4FmhHx3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_hdd is the original DataFrame and cluster_labels_hdd contains the cluster assignments\n",
        "df_hdd['cluster_hdd'] = cluster_labels_hdd\n",
        "\n",
        "# Calculating centroids for HDD data\n",
        "#HDD_centroids = df_hdd.groupby('cluster_hdd').mean()\n",
        "\n",
        "#print(HDD_centroids)"
      ],
      "metadata": {
        "id": "pSx_VLkiHyBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REQUEST AND RECIEVE INFORMATION\n",
        "\n"
      ],
      "metadata": {
        "id": "eBNFiynNtpIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"pull benchmarkscore from website\"\n",
        "def get_benchmark_score(component_model, dataset):\n",
        "    \"\"\"\n",
        "    Retrieves the benchmark score for a given component model from the dataset.\n",
        "\n",
        "    Args:\n",
        "    - component_model (str): The model identifier of the component.\n",
        "    - dataset (pd.DataFrame): The dataset containing component data.\n",
        "\n",
        "    Returns:\n",
        "    - float: The benchmark score of the component.\n",
        "    \"\"\"\n",
        "    component_data = dataset[dataset['Model'] == component_model]\n",
        "    if not component_data.empty:\n",
        "        return component_data.iloc[0]['Benchmark']\n",
        "    else:\n",
        "        print(f\"Component {component_model} not found in dataset.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "GY91iHKBeyyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"find the match based on model name of the component \"\n",
        "def find_closest_matches(target_scores, dataset):\n",
        "    \"\"\"\n",
        "    Finds the closest matches in the dataset for the target benchmark scores.\n",
        "\n",
        "    Args:\n",
        "    - target_scores (list of floats): Target benchmark scores for recommendations.\n",
        "    - dataset (pd.DataFrame): The dataset containing component data.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary mapping target increases to the closest matching components.\n",
        "    \"\"\"\n",
        "    recommendations = {}\n",
        "    for target in target_scores:\n",
        "        # Find components with benchmark scores closest to but not below the target\n",
        "        matches = dataset[dataset['Benchmark'] >= target].nsmallest(1, 'Benchmark')\n",
        "        if not matches.empty:\n",
        "            model = matches.iloc[0]['Model']\n",
        "            score = matches.iloc[0]['Benchmark']\n",
        "            recommendations[target] = (model, score)\n",
        "        else:\n",
        "            recommendations[target] = (\"No match found\", 0)\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "j5N-e0h8en6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU Recommendation method\n"
      ],
      "metadata": {
        "id": "4L3cghqoXrcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_CPU_component(start_component_name, increase_percents=[15, 30, 45]):\n",
        "    global df_cpu  # Assuming df_cpu is globally accessible\n",
        "\n",
        "    print(f\"Received model name from another method: '{start_component_name}'\")\n",
        "    start_component_matches = df_cpu[df_cpu['Model'] == start_component_name]\n",
        "    if start_component_matches.empty:\n",
        "        print(f\"Model '{start_component_name}' not found in DataFrame.\")\n",
        "        return []\n",
        "\n",
        "    start_component = start_component_matches.iloc[0]\n",
        "    start_benchmark = float(start_component['Benchmark'])\n",
        "    start_cluster = start_component['cluster']\n",
        "    print(f\"Found starting component '{start_component_name}' with Benchmark {start_benchmark} in cluster {start_cluster}.\")\n",
        "\n",
        "    target_benchmarks = [start_benchmark * (1 + p / 100) for p in increase_percents]\n",
        "    recommendations = []\n",
        "    cluster_components = df_cpu[df_cpu['cluster'] == start_cluster]\n",
        "\n",
        "    print(f\"Number of components in the same cluster: {len(cluster_components)}\")\n",
        "\n",
        "    for increase_percent, target in zip(increase_percents, target_benchmarks):\n",
        "        eligible = cluster_components[cluster_components['Benchmark'] >= target]\n",
        "        print(f\"Looking for components with at least {increase_percent}% increase (target benchmark: {target}). Eligible components: {len(eligible)}\")\n",
        "\n",
        "        if not eligible.empty:\n",
        "            closest = eligible.iloc[(eligible['Benchmark'] - target).abs().argsort()[:1]]\n",
        "            recommendation = closest[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "        else:\n",
        "            max_benchmark_component = df_cpu.loc[df_cpu['Benchmark'].idxmax()]\n",
        "            print(f\"No components found for a {increase_percent}% increase. The highest benchmark score in the dataset is by {max_benchmark_component['Model']} with a Benchmark of {max_benchmark_component['Benchmark']}.\")\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": {'Model': max_benchmark_component['Model'], 'Benchmark': max_benchmark_component['Benchmark']}})\n",
        "\n",
        "    # For debugging and verification, display the recommendations more clearly\n",
        "    for rec in recommendations:\n",
        "        print(f\"Recommendation for {rec['Increase']}% increase: Model '{rec['Details']['Model']}' with Benchmark {rec['Details']['Benchmark']}\")\n",
        "\n",
        "    return recommendations\n",
        "\n"
      ],
      "metadata": {
        "id": "gDMraDA_XuQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU recommendation method"
      ],
      "metadata": {
        "id": "zS-EmYfcXvff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_GPU_component(start_component_name, increase_percents=[15, 30, 45]):\n",
        "    global df_gpu  # Assuming df_gpu is globally accessible\n",
        "\n",
        "    # Check if the starting GPU model is in the DataFrame\n",
        "    start_component_matches = df_gpu[df_gpu['Model'] == start_component_name]\n",
        "    if start_component_matches.empty:\n",
        "        print(f\"GPU Model '{start_component_name}' not found in DataFrame.\")\n",
        "        return []\n",
        "\n",
        "    # Extract details of the starting component\n",
        "    start_component = start_component_matches.iloc[0]\n",
        "    start_benchmark = float(start_component['Benchmark'])\n",
        "    if 'cluster' in start_component:\n",
        "        start_cluster = start_component['cluster']\n",
        "        print(f\"Found starting GPU '{start_component_name}' with Benchmark {start_benchmark} in cluster {start_cluster}.\")\n",
        "    else:\n",
        "        start_cluster = None\n",
        "        print(f\"Cluster information is not available for '{start_component_name}'.\")\n",
        "\n",
        "    target_benchmarks = [start_benchmark * (1 + p / 100) for p in increase_percents]\n",
        "    recommendations = []\n",
        "\n",
        "    # Filter by cluster if available, else consider the whole dataset\n",
        "    cluster_components = df_gpu[df_gpu['cluster'] == start_cluster] if start_cluster else df_gpu\n",
        "\n",
        "    for increase_percent, target in zip(increase_percents, target_benchmarks):\n",
        "        eligible = cluster_components[cluster_components['Benchmark'] >= target]\n",
        "        print(f\"Looking for GPUs with at least {increase_percent}% increase (target benchmark: {target}). Eligible components: {len(eligible)}\")\n",
        "\n",
        "        if not eligible.empty:\n",
        "            closest = eligible.iloc[(eligible['Benchmark'] - target).abs().argsort()[:1]]\n",
        "            recommendation = closest[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "        else:\n",
        "            max_benchmark_component = df_gpu.loc[df_gpu['Benchmark'].idxmax()]\n",
        "            print(f\"No GPUs found for a {increase_percent}% increase. The highest benchmark score in the dataset is by {max_benchmark_component['Model']} with a Benchmark of {max_benchmark_component['Benchmark']}.\")\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": {'Model': max_benchmark_component['Model'], 'Benchmark': max_benchmark_component['Benchmark']}})\n",
        "\n",
        "    # For debugging and verification, clearly display the recommendations\n",
        "    for rec in recommendations:\n",
        "        print(f\"Recommendation for {rec['Increase']}% increase: Model '{rec['Details']['Model']}' with Benchmark {rec['Details']['Benchmark']}\")\n",
        "\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "mCJsLfreXypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM Recommendation method"
      ],
      "metadata": {
        "id": "27qo2EC9XzAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_RAM_component(start_component_name, increase_percents=[15, 30, 45]):\n",
        "    global df_ram  # Assuming df_ram is globally accessible\n",
        "\n",
        "    # Check if the starting RAM model exists in the DataFrame\n",
        "    start_component_matches = df_ram[df_ram['Model'] == start_component_name]\n",
        "    if start_component_matches.empty:\n",
        "        print(f\"RAM Model '{start_component_name}' not found in DataFrame.\")\n",
        "        return []\n",
        "\n",
        "    # Extract details of the starting component\n",
        "    start_component = start_component_matches.iloc[0]\n",
        "    start_benchmark = float(start_component['Benchmark'])\n",
        "    start_cluster = start_component.get('cluster_ram', None)\n",
        "    print(f\"Found starting RAM component '{start_component_name}' with Benchmark {start_benchmark} in cluster {start_cluster}.\")\n",
        "\n",
        "    target_benchmarks = [start_benchmark * (1 + p / 100) for p in increase_percents]\n",
        "    recommendations = []\n",
        "\n",
        "    cluster_components = df_ram[df_ram['cluster_ram'] == start_cluster] if start_cluster else df_ram\n",
        "\n",
        "    for increase_percent, target in zip(increase_percents, target_benchmarks):\n",
        "        eligible_within_cluster = cluster_components[cluster_components['Benchmark'] >= target]\n",
        "\n",
        "        if not eligible_within_cluster.empty:\n",
        "            # Identify the closest match within the same cluster\n",
        "            closest_within_cluster = eligible_within_cluster.iloc[(eligible_within_cluster['Benchmark'] - target).abs().argsort()[:1]]\n",
        "            recommendation = closest_within_cluster[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "        else:\n",
        "            # Search the entire dataset if no suitable component within the cluster\n",
        "            all_eligible = df_ram[df_ram['Benchmark'] >= target]\n",
        "            if not all_eligible.empty:\n",
        "                closest_across_all = all_eligible.iloc[(all_eligible['Benchmark'] - target).abs().argsort()[:1]]\n",
        "                recommendation = closest_across_all[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "                recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "            else:\n",
        "                print(f\"No RAM components found for a {increase_percent}% increase.\")\n",
        "                # Consider adding a default case here if needed, such as recommending the highest benchmark RAM in the dataset or within the cluster\n",
        "\n",
        "    # Print the recommendations with the structured format\n",
        "    for rec in recommendations:\n",
        "        print(f\"Recommendation for {rec['Increase']}% increase: Model '{rec['Details']['Model']}' with Benchmark {rec['Details']['Benchmark']}\")\n",
        "\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "SKyvT4qDX1cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SSD recommendation method"
      ],
      "metadata": {
        "id": "bx75JXksX1vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_SSD_component(start_component_name, increase_percents=[15, 30, 45]):\n",
        "    global df_ssd  # Access df_ssd globally within the function\n",
        "\n",
        "    # Check if the starting SSD model exists in the DataFrame\n",
        "    start_component_matches = df_ssd[df_ssd['Model'] == start_component_name]\n",
        "    if start_component_matches.empty:\n",
        "        print(f\"SSD Model '{start_component_name}' not found in DataFrame.\")\n",
        "        return []\n",
        "\n",
        "    # Extract details of the starting component\n",
        "    start_component = start_component_matches.iloc[0]\n",
        "    start_benchmark = float(start_component['Benchmark'])  # Ensure start_benchmark is a float\n",
        "    start_cluster = start_component['cluster_ssd']\n",
        "    print(f\"Found starting SSD component '{start_component_name}' with Benchmark {start_benchmark} in cluster {start_cluster}.\")\n",
        "\n",
        "    # Calculate target benchmarks for the desired performance increases\n",
        "    target_benchmarks = [start_benchmark * (1 + p / 100) for p in increase_percents]\n",
        "    recommendations = []\n",
        "\n",
        "    # Search for eligible components within the same cluster\n",
        "    cluster_components = df_ssd[df_ssd['cluster_ssd'] == start_cluster]\n",
        "\n",
        "    for increase_percent, target in zip(increase_percents, target_benchmarks):\n",
        "        print(f\"Looking for SSD components with at least {increase_percent}% increase (target benchmark: {target}).\")\n",
        "        eligible_within_cluster = cluster_components[cluster_components['Benchmark'] >= target]\n",
        "        print(f\"Eligible components within the same cluster: {len(eligible_within_cluster)}\")\n",
        "\n",
        "        if not eligible_within_cluster.empty:\n",
        "            closest_within_cluster = eligible_within_cluster.iloc[(eligible_within_cluster['Benchmark'] - target).abs().argsort()[:1]]\n",
        "            recommendation = closest_within_cluster[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "        else:\n",
        "            all_eligible = df_ssd[df_ssd['Benchmark'] >= target]\n",
        "            print(f\"Eligible components across all clusters: {len(all_eligible)}\")\n",
        "\n",
        "            if not all_eligible.empty:\n",
        "                closest_across_all = all_eligible.iloc[(all_eligible['Benchmark'] - target).abs().argsort()[:1]]\n",
        "                recommendation = closest_across_all[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "                recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "            else:\n",
        "                print(f\"No SSD components found for a {increase_percent}% increase. The highest benchmark score in the cluster is by {start_component_name} with a Benchmark of {start_benchmark}.\")\n",
        "                recommendation = {'Model': start_component_name, 'Benchmark': start_benchmark}\n",
        "                recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "\n",
        "    # Print the recommendations\n",
        "    for rec in recommendations:\n",
        "        print(f\"Recommendation for {rec['Increase']}% increase: Model '{rec['Details']['Model']}' with Benchmark {rec['Details']['Benchmark']}\")\n",
        "\n",
        "    return recommendations\n",
        "\n"
      ],
      "metadata": {
        "id": "rUFvEqG6X5iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HDD recommendation method"
      ],
      "metadata": {
        "id": "hprTV5ySX56j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_HDD_component(start_component_name, increase_percents=[15, 30, 45]):\n",
        "    global df_hdd  # Access df_hdd globally within the function\n",
        "\n",
        "    # Verify if the starting HDD model is in the DataFrame\n",
        "    start_component_matches = df_hdd[df_hdd['Model'] == start_component_name]\n",
        "    if start_component_matches.empty:\n",
        "        print(f\"HDD Model '{start_component_name}' not found in DataFrame.\")\n",
        "        return []\n",
        "\n",
        "    # Extract details of the starting component\n",
        "    start_component = start_component_matches.iloc[0]\n",
        "    start_benchmark = float(start_component['Benchmark'])  # Ensure start_benchmark is a float\n",
        "    start_cluster = start_component['cluster_hdd']\n",
        "    print(f\"Found starting HDD component '{start_component_name}' with Benchmark {start_benchmark} in cluster {start_cluster}.\")\n",
        "\n",
        "    # Calculate target benchmarks for the desired performance increases\n",
        "    target_benchmarks = [start_benchmark * (1 + p / 100) for p in increase_percents]\n",
        "    recommendations = []\n",
        "\n",
        "    # Search for eligible components within the same cluster\n",
        "    cluster_components = df_hdd[df_hdd['cluster_hdd'] == start_cluster]\n",
        "\n",
        "    for increase_percent, target in zip(increase_percents, target_benchmarks):\n",
        "        print(f\"Looking for HDD components with at least {increase_percent}% increase (target benchmark: {target}).\")\n",
        "        eligible_within_cluster = cluster_components[cluster_components['Benchmark'] >= target]\n",
        "        print(f\"Eligible components within the same cluster: {len(eligible_within_cluster)}\")\n",
        "\n",
        "        if not eligible_within_cluster.empty:\n",
        "            closest_within_cluster = eligible_within_cluster.iloc[(eligible_within_cluster['Benchmark'] - target).abs().argsort()[:1]]\n",
        "            recommendation = closest_within_cluster[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "            recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "        else:\n",
        "            all_eligible = df_hdd[df_hdd['Benchmark'] >= target]\n",
        "            print(f\"Eligible components across all clusters: {len(all_eligible)}\")\n",
        "\n",
        "            if not all_eligible.empty:\n",
        "                closest_across_all = all_eligible.iloc[(all_eligible['Benchmark'] - target).abs().argsort()[:1]]\n",
        "                recommendation = closest_across_all[['Model', 'Benchmark']].to_dict('records')[0]\n",
        "                recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "            else:\n",
        "                print(f\"No HDD components found for a {increase_percent}% increase. The highest benchmark score in the cluster is by {start_component_name} with a Benchmark of {start_benchmark}.\")\n",
        "                # Default to the highest benchmark score within the same cluster if no better options are found\n",
        "                recommendation = {'Model': start_component_name, 'Benchmark': start_benchmark}\n",
        "                recommendations.append({\"Increase\": increase_percent, \"Details\": recommendation})\n",
        "\n",
        "    # Print the recommendations\n",
        "    for rec in recommendations:\n",
        "        print(f\"Recommendation for {rec['Increase']}% increase: Model '{rec['Details']['Model']}' with Benchmark {rec['Details']['Benchmark']}\")\n",
        "\n",
        "    return recommendations\n",
        "\n"
      ],
      "metadata": {
        "id": "D3vGhcjHX8HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import pubsub_v1\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import logging\n",
        "import threading\n",
        "\n",
        "# Initialize Pub/Sub client\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "\n",
        "# Define the topic name where recommendations will be published\n",
        "output_topic_name = 'projects/watchful-net-416319/topics/PC_Building_Assistant_Receiver'\n",
        "\n",
        "\n",
        "recommendation_methods = {\n",
        "    'CPU': recommend_CPU_component,\n",
        "    'GPU': recommend_GPU_component,\n",
        "    'RAM': recommend_RAM_component,\n",
        "    'SSD': recommend_SSD_component,\n",
        "    'HDD': recommend_HDD_component\n",
        "}\n",
        "\n",
        "# Initialize the Pub/Sub client\n",
        "subscriber = pubsub_v1.SubscriberClient()\n",
        "\n",
        "# Your Google Cloud project ID\n",
        "project_id = \"watchful-net-416319\"\n",
        "\n",
        "# Existing subscription details\n",
        "existing_subscription_id = \"PC_Building_Assistant\"\n",
        "existing_subscription_path = subscriber.subscription_path(project_id, existing_subscription_id)\n",
        "\n",
        "# New subscription details for PC_Building_Assistant_Receiver\n",
        "new_subscription_id = \"PC_Building_Assistant_Receiver\"\n",
        "new_subscription_path = subscriber.subscription_path(project_id, new_subscription_id)\n",
        "\n",
        "function_url ='https://us-central1-watchful-net-416319.cloudfunctions.net/PC_Building_Assistant_Receiver'\n",
        "\n",
        "def callback_existing(message):\n",
        "    try:\n",
        "        print(f\"Received message on {existing_subscription_id}: {message}\")\n",
        "        process_message(message)\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError occurred: {str(e)} - Possible missing data in message.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {str(e)}\")\n",
        "    finally:\n",
        "        message.ack()  # Acknowledge the message to prevent further retries\n",
        "\n",
        "\n",
        "def callback_new(message):\n",
        "    \"\"\"Callback function for the new subscription.\"\"\"\n",
        "    print(f\"Received message on {new_subscription_id}: {message}\")\n",
        "    # Process the message similarly as before\n",
        "    process_message(message)\n",
        "\n",
        "def process_message(message):\n",
        "    \"\"\"Process message common logic.\"\"\"\n",
        "    data = message.data.decode(\"utf-8\")\n",
        "    payload = json.loads(data)\n",
        "    model = payload.get(\"model\")\n",
        "    componentType = payload.get(\"componentType\", payload.get(\"type\"))  # Fallback to \"type\" if \"componentType\" is missing\n",
        "    email = payload.get(\"email\")  # Assuming email is correctly provided in the message\n",
        "    username = payload.get(\"username\")  # Correctly retrieve username from the message\n",
        "\n",
        "    if not componentType:\n",
        "        print(f\"Error: Component type is missing or None for model {model}. Message payload: {payload}\")\n",
        "        return  # Exit the function if component_type is missing or None\n",
        "\n",
        "    if componentType in recommendation_methods:\n",
        "        print(f\"Processing recommendation for Model: {model}, Component Type: {componentType}\")\n",
        "        print(f\"User: {username}, Email: {email}\")  # Log username and email\n",
        "\n",
        "        # Assume recommendation_methods[component_type](model) is defined elsewhere\n",
        "        recommendation_result = recommendation_methods[componentType](model)\n",
        "\n",
        "        recommendation_message = {\n",
        "            \"model\": model,\n",
        "            \"componentType\": componentType,\n",
        "            \"recommendation\": recommendation_result,\n",
        "            \"email\": email,\n",
        "            \"username\": username  # Correctly include username in the output message\n",
        "        }\n",
        "\n",
        "        publish_recommendation(recommendation_message)\n",
        "    else:\n",
        "        print(f\"Unknown component type: {componentType}\")\n",
        "\n",
        "def publish_recommendation(recommendation):\n",
        "    \"\"\"Sends the recommendation to the Cloud Function via HTTP POST.\"\"\"\n",
        "    try:\n",
        "        headers = {'Content-Type': 'application/json'}\n",
        "        print(\"Attempting to send recommendation...\")\n",
        "\n",
        "        # Send the POST request to the Cloud Function endpoint\n",
        "        response = requests.post(function_url, data=json.dumps(recommendation), headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"Recommendation sent to Cloud Function successfully.\")\n",
        "        else:\n",
        "            print(f\"Failed to send recommendation, status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while sending the recommendation: {e}\")\n",
        "\n",
        "\n",
        "# Subscribe to the existing subscription\n",
        "subscriber_future_existing = subscriber.subscribe(existing_subscription_path, callback_existing)\n",
        "threading.Thread(target=lambda: subscriber_future_existing.result()).start()\n",
        "\n",
        "# Subscribe to the new subscription\n",
        "subscriber_future_new = subscriber.subscribe(new_subscription_path, callback_new)\n",
        "threading.Thread(target=lambda: subscriber_future_new.result()).start()\n",
        "\n",
        "print(f\"Listening for messages on {existing_subscription_id} and {new_subscription_id}...\\n\")\n",
        "\n",
        "try:\n",
        "    # Keep the main thread running\n",
        "    while True:\n",
        "        pass\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Interrupted, stopping...\")\n",
        "    subscriber_future_existing.cancel()\n",
        "    subscriber_future_new.cancel()\n",
        "import time\n",
        "while True:\n",
        "    time.sleep(60)\n"
      ],
      "metadata": {
        "id": "hdzdLrhsGbYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}